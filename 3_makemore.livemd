<!-- livebook:{"persist_outputs":true} -->

# Makemore #2 (Karpathy Neural Networks Series #3)

```elixir
Mix.install(
  [
    {:nx, "~> 0.5.1"},
    {:exla, "~> 0.5.1"},
    {:axon, "~> 0.5.1"},
    {:kino, "~> 0.8.1"},
    {:vega_lite, "~> 0.1.6"},
    {:kino_vega_lite, "~> 0.1.7"}
  ],
  system_env: [
    # {"XLA_TARGET", "cuda118"},
  ],
  config: [
    nx: [default_backend: EXLA.Backend]
  ]
)
```

<!-- livebook:{"output":true} -->

```
:ok
```

## Section

```elixir
# https://www.youtube.com/watch?v=PaCmpygFfXo&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=2

words =
  Path.join(__DIR__, 'names.txt')
  |> File.read!()
  |> String.split("\n")

stoi =
  ["." | words]
  |> Enum.flat_map(&String.graphemes/1)
  |> MapSet.new()
  |> Enum.sort()
  |> Enum.with_index()
  |> Map.new()

itos = Map.new(stoi, fn {s, i} -> {i, s} end)

Enum.slice(words, 0..7)
```

<!-- livebook:{"output":true} -->

```
["emma", "olivia", "ava", "isabella", "sophia", "charlotte", "mia", "amelia"]
```

```elixir
defmodule OneHot do
  import Nx.Defn
  defn(enc(t), do: t |> Nx.new_axis(-1) |> Nx.equal(Nx.iota({27})))
end
```

<!-- livebook:{"output":true} -->

```
{:module, OneHot, <<70, 79, 82, 49, 0, 0, 9, ...>>, true}
```

```elixir
defmodule Dataset do
  def build(words, stoi, block_size \\ 3) do
    build_non_enc(words, stoi, block_size)
    |> Enum.map(&OneHot.enc/1)
  end

  def build_non_enc(words, stoi, block_size \\ 3) do
    for w <- words,
        chs <- [String.graphemes("#{String.duplicate(".", block_size)}#{w}.")],
        chs <- Enum.chunk_every(chs, block_size + 1, 1, :discard) do
      is = chs |> Enum.map(&stoi[&1])
      {xs, [y]} = Enum.split(is, block_size)
      {xs, y}
    end
    |> Enum.unzip()
    |> Tuple.to_list()
    |> Enum.map(&Nx.tensor(&1, type: :u8))
  end
end

[xs, ys] = Dataset.build_non_enc(words, stoi, 3)
[xenc, yenc] = Dataset.build(words, stoi, 3)

# training, dev, test splits
# 80%, 10%, 10%
# train parameters, train hyper-parameters, evaluate at end
# only test on test set few times (otherwise you will start overfitting on that as well)

block_size = 10
wc = length(words)
n1 = floor(0.8 * wc)
n2 = floor(0.9 * wc)
[xtr, ytr] = Dataset.build(Enum.slice(words, 0..(n1 - 1)), stoi, block_size)
[xdev, ydev] = Dataset.build(Enum.slice(words, n1..(n2 - 1)), stoi, block_size)
[xte, yte] = Dataset.build(Enum.slice(words, n2..-1), stoi, block_size)

{Nx.shape(xtr), Nx.shape(xdev), Nx.shape(xte)}
```

<!-- livebook:{"output":true} -->

```
{{182778, 10, 27}, {22633, 10, 27}, {22735, 10, 27}}
```

```elixir
alias Axon, as: A
alias VegaLite, as: Vl

train_data =
  Stream.zip(
    Nx.to_batched(xtr, 1024),
    Nx.to_batched(ytr, 1024)
  )

model =
  A.input("context")
  |> A.dense(20, activation: nil, use_bias: false, name: "C")
  |> A.flatten()
  |> A.dense(100, activation: :tanh, use_bias: true, name: "W1")
  |> A.dense(27, activation: :softmax, use_bias: true, name: "W2")

plot =
  Vl.new()
  |> Vl.mark(:line)
  |> Vl.encode_field(:x, "step", type: :quantitative)
  |> Vl.encode_field(:y, "loss", type: :quantitative)
  |> Kino.VegaLite.new()
  |> Kino.render()

trained = %{}

trained =
  model
  |> A.Loop.trainer(:categorical_cross_entropy, A.Optimizers.adam(0.01))
  |> A.Loop.kino_vega_lite_plot(plot, "loss")
  |> A.Loop.run(train_data, trained, iterations: 100_000, epochs: 5, compiler: EXLA)

# Decay learning rate
trained =
  model
  |> A.Loop.trainer(:categorical_cross_entropy, A.Optimizers.adam(0.001))
  |> A.Loop.kino_vega_lite_plot(plot, "loss")
  |> A.Loop.run(train_data, trained, iterations: 100_000, epochs: 5, compiler: EXLA)

:ok
```

<!-- livebook:{"output":true} -->

```

15:11:38.973 [debug] Forwarding options: [compiler: EXLA] to JIT compiler
Epoch: 0, Batch: 175, loss: 2.2649834
Epoch: 1, Batch: 171, loss: 2.2050734
Epoch: 2, Batch: 167, loss: 2.1692605
Epoch: 3, Batch: 163, loss: 2.1425097
Epoch: 4, Batch: 159, loss: 2.1206124

15:11:42.254 [debug] Forwarding options: [compiler: EXLA] to JIT compiler
Epoch: 0, Batch: 175, loss: 2.0086689
Epoch: 1, Batch: 171, loss: 1.9949518
Epoch: 2, Batch: 167, loss: 1.9879596
Epoch: 3, Batch: 163, loss: 1.9832108
Epoch: 4, Batch: 159, loss: 1.9792726
```

<!-- livebook:{"output":true} -->

```
:ok
```

```elixir
# manual forward pass and loss

# %{
#   "C" => %{"kernel" => c},
#   "W1" => %{"kernel" => w1, "bias" => b1},
#   "W2" => %{"kernel" => w2, "bias" => b2}
# } = trained

# emb = xdev |> Nx.dot(c) |> Nx.reshape({:auto, 6})
# h = Nx.tanh(emb |> Nx.dot(w1) |> Nx.add(b1))
# logits = h |> Nx.dot(w2) |> Nx.add(b2)
# counts = logits |> Nx.exp()
# probs = counts |> Nx.divide(Nx.sum(counts, axes: [1], keep_axes: true))

ytr_pred = Axon.predict(model, trained, xtr)

Axon.Losses.categorical_cross_entropy(ytr, ytr_pred, reduction: :mean)
|> Nx.to_number()
|> IO.inspect(label: :overall_nll_train)

ydev_pred = Axon.predict(model, trained, xdev)

Axon.Losses.categorical_cross_entropy(ydev, ydev_pred, reduction: :mean)
|> Nx.to_number()
|> IO.inspect(label: :overall_nll_dev)

# # sample first letter of name
first_letter_context = xtr[0..0]

for i <- 1..20 do
  key = Nx.Random.key(i)
  probs = Axon.predict(model, trained, first_letter_context)
  {sample, _} = Nx.Random.choice(key, Nx.iota({27}), probs[0])
  itos[sample |> Nx.reshape({}) |> Nx.to_number()]
end
```

<!-- livebook:{"output":true} -->

```
overall_nll_train: 2.0180282592773438
overall_nll_dev: 2.206791639328003
```

<!-- livebook:{"output":true} -->

```
["s", "k", "k", "c", "b", "s", "v", "k", "l", "c", "j", "d", "r", "a", "j", "a", "j", "a", "a", "b"]
```

```elixir
# 2D visualization only meaningful for "C" matrix with width 2
c = trained["C"]["kernel"]

data =
  for i <- 0..26, do: %{x: c[i][0] |> Nx.to_number(), y: c[i][1] |> Nx.to_number(), text: itos[i]}

Vl.new(width: 400, height: 400)
|> Vl.data_from_values(data)
|> Vl.mark(:text)
|> Vl.encode_field(:x, "x", type: :quantitative)
|> Vl.encode_field(:y, "y", type: :quantitative)
|> Vl.encode_field(:text, "text", type: :nominal)
```

<!-- livebook:{"output":true} -->

```vega-lite
{"$schema":"https://vega.github.io/schema/vega-lite/v5.json","data":{"values":[{"text":".","x":0.059716563671827316,"y":0.04999270290136337},{"text":"a","x":-0.7174668312072754,"y":-0.05367143079638481},{"text":"b","x":0.12739774584770203,"y":-0.7587493062019348},{"text":"c","x":-0.3461497128009796,"y":-0.30487188696861267},{"text":"d","x":-0.5564382076263428,"y":-0.6050490736961365},{"text":"e","x":-0.6826889514923096,"y":-0.2763334810733795},{"text":"f","x":0.2371005117893219,"y":-0.08513400703668594},{"text":"g","x":-0.43029457330703735,"y":-0.2636324465274811},{"text":"h","x":0.42847734689712524,"y":-0.7184445858001709},{"text":"i","x":-0.2899906635284424,"y":-0.11618417501449585},{"text":"j","x":-0.08513091504573822,"y":-0.7006126642227173},{"text":"k","x":-0.3286883533000946,"y":-0.38406187295913696},{"text":"l","x":-0.26709938049316406,"y":-0.26666316390037537},{"text":"m","x":0.05384950712323189,"y":-1.4821873903274536},{"text":"n","x":0.10948823392391205,"y":0.032672811299562454},{"text":"o","x":-0.20969252288341522,"y":0.556032657623291},{"text":"p","x":0.1967368870973587,"y":-0.18692323565483093},{"text":"q","x":-0.16911493241786957,"y":1.0665138959884644},{"text":"r","x":-0.6547238230705261,"y":-0.5146574974060059},{"text":"s","x":-0.3565792441368103,"y":-0.1522601842880249},{"text":"t","x":0.09015679359436035,"y":0.29228535294532776},{"text":"u","x":-1.3054821491241455,"y":-0.09678894281387329},{"text":"v","x":0.5612456798553467,"y":-1.242600679397583},{"text":"w","x":-0.17013682425022125,"y":-0.9160974025726318},{"text":"x","x":-0.38454994559288025,"y":0.39319226145744324},{"text":"y","x":-1.053345799446106,"y":0.20891374349594116},{"text":"z","x":-0.7373540997505188,"y":-0.18748295307159424}]},"encoding":{"text":{"field":"text","type":"nominal"},"x":{"field":"x","type":"quantitative"},"y":{"field":"y","type":"quantitative"}},"height":400,"mark":"text","width":400}
```

```elixir
defmodule SampleAxon do
  def take(predict_fn, itos, block_size, seed \\ :rand.uniform(9_999_999)) do
    word(predict_fn, Nx.Random.key(seed), block_size, List.duplicate(0, block_size))
    |> Enum.map(&itos[&1])
    |> Enum.join()
    |> String.trim(".")
  end

  defp word(predict_fn, key, block_size, acc)
  defp word(_, _, _, acc) when length(acc) > 100, do: Enum.reverse(acc)
  defp word(_, _, block_size, [0 | _] = acc) when length(acc) > block_size, do: Enum.reverse(acc)

  defp word(predict_fn, key, block_size, acc) do
    context = acc |> Enum.take(block_size) |> Enum.reverse()
    xenc = Nx.tensor(context) |> OneHot.enc() |> Nx.reshape({1, block_size, 27})
    p = predict_fn.(xenc)[0]
    {c, key} = Nx.Random.choice(key, Nx.iota({27}), p, samples: 1)
    c = c |> Nx.reshape({}) |> Nx.to_number()

    word(predict_fn, key, block_size, [c | acc])
  end
end

for i <- 1..10,
    do: SampleAxon.take(&Axon.predict(model, trained, &1), itos, block_size, i + 424_242)
```

<!-- livebook:{"output":true} -->

```
["kasen", "raivon", "cash", "asantly", "dhamrien", "payton", "evanda", "catree", "jazellen",
 "letin"]
```
