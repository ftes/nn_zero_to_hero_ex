# Micrograd (Karpathy Neural Networks Series #1) - fork

```elixir
Mix.install(
  [
    {:axon, "~> 0.5.1"},
    {:nx, "~> 0.5"},
    {:vega_lite, "~> 0.1.6"},
    {:kino_vega_lite, "~> 0.1.7"}
  ],
  consolidate_protocols: false
)
```

## Derivatives

```elixir
# https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ

f = fn x -> 3 * x * x - 4 * x + 5 end
f.(3.0) |> IO.inspect()

alias VegaLite, as: Vl

# floating point range (step size < 1)
defmodule FRange do
  def new(from, to, step) do
    Stream.unfold(from, fn
      x when x > to -> nil
      x -> {x, x + step}
    end)
  end
end

Vl.new()
|> Vl.data_from_values(for x <- FRange.new(-5, 5, 0.1), do: %{"x" => x, y: f.(x)})
|> Vl.mark(:line)
|> Vl.encode_field(:x, "x", type: :quantitative)
|> Vl.encode_field(:y, "y", type: :quantitative)
|> Kino.render()

:ok
```

```elixir
h = 0.000001
x = 2 / 3
(f.(x + h) - f.(x)) / h
```

```elixir
a = 2.0
b = -3.0
c = 10.0
d = a * b + c
d |> IO.puts()
```

```elixir
h = 0.0001

# inputs
a = 2.0
b = -3.0
c = 10.0
d1 = a * b + c
a = a + h
d2 = a * b + c
d1 |> IO.puts()
d2 |> IO.puts()
((d2 - d1) / h) |> IO.inspect(label: :slope_a)

d1 = a * b + c
b = b + h
d2 = a * b + c
# d1 |> IO.puts()
# d2 |> IO.puts()
((d2 - d1) / h) |> IO.inspect(label: :slope_b)

d1 = a * b + c
c = c + h
d2 = a * b + c
# d1 |> IO.puts()
# d2 |> IO.puts()
((d2 - d1) / h) |> IO.inspect(label: :slope_c)
:ok
```

```elixir
defmodule Value do
  import Kernel, except: [+: 2, -: 2, /: 2, *: 2, **: 2]

  defmacro __using__(_) do
    quote do
      import Kernel, except: [+: 2, -: 2, /: 2, *: 2, **: 2]
      import Value
    end
  end

  @derive {Inspect, only: [:data]}
  defstruct [:data, :label, :id, prev: [], op: nil, grad: 0.0]

  def new(data, label)
  def new(data, label), do: %Value{data: data, label: label} |> new()
  def new(%Value{} = v), do: %{v | id: :erlang.unique_integer()}

  def sum([%Value{} = v | vs]),
    do: Enum.reduce(vs, v, fn v, acc -> (v + acc) |> Value.label("+") end)

  def (%Value{} = a) + b when is_number(b), do: a + new(b, "#{b}")
  def a + (%Value{} = b) when is_number(a), do: new(a, "#{a}") + b

  def (%Value{} = a) + (%Value{} = b) do
    %Value{
      data: a.data + b.data,
      label: "+",
      prev: [a, b],
      op: :+
    }
    |> new()
  end

  def a + b, do: Kernel.+(a, b)

  def (%Value{} = a) * b when is_number(b), do: a * new(b, "#{b}")
  def a * (%Value{} = b) when is_number(a), do: new(a, "#{a}") * b

  def (%Value{} = a) * (%Value{} = b) do
    %Value{
      data: a.data * b.data,
      label: "x",
      prev: [a, b],
      op: :x
    }
    |> new()
  end

  def a * b, do: Kernel.*(a, b)

  def tanh(%Value{} = v) do
    %Value{
      data: (:math.exp(2 * v.data) - 1) / (:math.exp(2 * v.data) + 1),
      label: "tanh",
      prev: [v],
      op: :tanh
    }
    |> new()
  end

  def exp(v) when is_number(v), do: exp(new(v, "#{v}"))

  def exp(%Value{} = v) do
    %Value{
      data: :math.exp(v.data),
      label: "exp",
      prev: [v],
      op: :exp
    }
    |> new()
  end

  def (%Value{} = a) ** b when is_number(b) do
    %Value{
      data: a.data ** b,
      label: "**",
      prev: [a, new(b, "#{b}")],
      op: :pow
    }
    |> new()
  end

  def a ** b, do: Kernel.**(a, b)

  def (%Value{} = a) / b when is_number(b), do: a / new(b, "#{b}")
  def a / (%Value{} = b) when is_number(a), do: new(a, "#{a}") / b
  def (%Value{} = a) / (%Value{} = b), do: (a * b ** -1.0) |> label("div")
  def a / b, do: Kernel./(a, b)

  def neg(%Value{} = v), do: (v * -1.0) |> label("neg")

  def (%Value{} = a) - b when is_number(b), do: a - new(b, "#{b}")
  def (%Value{} = a) - (%Value{} = b), do: (a + neg(b)) |> label("-")
  def a - b, do: Kernel.-(a, b)

  def label(%Value{} = v, label), do: %{v | label: label}
  def grad(%Value{} = v, grad), do: %{v | grad: grad}
  def nudge(%Value{} = v, delta \\ 0.01), do: %{v | data: v.data + delta * v.grad}
end

defmodule Value.Kino do
  def new(%Value{} = v) do
    (["graph TB;"] ++ lines(v))
    |> Enum.join("\n")
    |> Kino.Mermaid.new()
  end

  defp lines(%{op: nil}), do: []

  defp lines(v) do
    [edge(node(:op, v), node(:data, v))] ++
      Enum.map(v.prev, &edge(node(:data, &1), node(:op, v))) ++
      Enum.flat_map(v.prev, &lines(&1))
  end

  def edge(a, b), do: "#{a} --> #{b};"

  def node(:data, v),
    do: "v_#{v.id}[#{v.label}: data #{Float.round(v.data, 2)}, grad #{Float.round(v.grad, 2)}]"

  def node(:op, v), do: "v_#{v.id}_op((#{v.op}))"
end

alias Value, as: V
```

```elixir
use Value

# 4. dL/da = dL/de * de/da = -2 * b = 6
a = V.new(2.0, "a") |> V.grad(6.0)
# 4. dL/db = dL/de * de/db = -2 * a = -4
b = V.new(-3.0, "b") |> V.grad(-4.0)
# 3. dL/dc = dL/dd * dd/dc = -2 * 1 = -2
c = V.new(10.0, "c") |> V.grad(-2.0)
# 2. dL/df = d = 4
f = V.new(-2.0, "f") |> V.grad(4.0)
# 3. dL/de = dL/dd * dd/de = -2 * 1 = -2
e = (a * b) |> V.label("e") |> V.grad(-2.0)
# 2. dL/dd = f = -2
d = (e + c) |> V.label("d") |> V.grad(-2.0)
# 1. dL/dL = 1
l = (d * f) |> V.label("L") |> V.grad(1.0) |> V.Kino.new()
```

```elixir
lol = fn ->
  h = 0.0001

  a = V.new(2.0, "a")
  b = V.new(-3.0, "b")
  c = V.new(10.0, "c")
  e = (a * b) |> V.label("e")
  d = (e * c) |> V.label("d")
  f = V.new(-2.0, "f")
  l = (d * f) |> V.label("L")
  l1 = l.data

  a = V.new(2.0 + h, "a")
  b = V.new(-3.0, "b")
  c = V.new(10.0, "c")
  e = (a * b) |> V.label("e")
  d = (e + c) |> V.label("d")
  f = V.new(-2.0, "f")
  l = (d * f) |> V.label("L")
  l2 = l.data

  (l2 - l1) / h
end

lol.() |> IO.inspect(label: :grad_a)
```

```elixir
a = V.new(2.0, "a") |> V.grad(6.0) |> V.nudge()
b = V.new(-3.0, "b") |> V.grad(-4.0) |> V.nudge()
c = V.new(10.0, "c") |> V.grad(-2.0) |> V.nudge()
f = V.new(-2.0, "f") |> V.grad(4.0) |> V.nudge()
e = (a * b) |> V.label("e")
d = (e + c) |> V.label("d")
l = (d * f) |> V.label("L")

IO.puts(l.data)
```

```elixir
Vl.new()
|> Vl.data_from_values(for x <- FRange.new(-5, 5, 0.1), do: %{"x" => x, y: :math.tanh(x)})
|> Vl.mark(:line)
|> Vl.encode_field(:x, "x", type: :quantitative)
|> Vl.encode_field(:y, "y", type: :quantitative)
```

```elixir
# inputs
# 5. w1.data * x1w1.grad
x1 = V.new(2.0, "x1") |> V.grad(-1.5)
# 5. do/dx2 = do/dx2*w2 * dx2*w2/dx2 = 0.5 * w2 = 0.5
x2 = V.new(0.0, "x2") |> V.grad(0.5)
# weights
# 5. x1.data * x1w1.grad
w1 = V.new(-3.0, "w1") |> V.grad(1.0)
# 5. do/dx2 = do/dx2*w2 * dx2*w2/dx2 = 0.5 * x2 = 0.
w2 = V.new(1.0, "w2") |> V.grad(0.0)
# bias
# 3. do/db after '+', just copy n.grad
b = V.new(6.8813735870195432, "b") |> V.grad(0.5)

# 4. do/x1w1 after '+', just copy x1*w1....grad
x1w1 = (x1 * w1) |> V.grad(0.5)
# 4. do/x1w1 after '+', just copy x1*w1....grad
x2w2 = (x2 * w2) |> V.grad(0.5)
# 3. do/dx1*... after '+', just copy n.grad
x1w1x2w2 = (x1w1 + x2w2) |> V.grad(0.5)
# 4. do/dn = 1-tanh(o)^2 = 1-0.7^2 = 0.5
n = (x1w1x2w2 + b) |> V.label("n") |> V.grad(0.5)
# 5. do/do = 1
o = n |> V.tanh() |> V.label("o") |> V.grad(1.0)
o |> V.Kino.new()
```

```elixir
defmodule BackpropagationWithBug do
  alias Value, as: V

  def backward(%V{} = v), do: b(%{v | grad: 1.0})

  defp b([_ | _] = values) do
    # Implicit topological sort via recursion. Sufficient?
    # TODO: track visited (break cycles)
    Enum.map(values, &b(&1))
  end

  defp b(%V{op: :+} = v) do
    [c1, c2] = v.prev
    c1 = %{c1 | grad: c1.grad + 1.0 * v.grad}
    c2 = %{c2 | grad: c2.grad + 1.0 * v.grad}
    %{v | prev: [c1, c2] |> b()}
  end

  defp b(%V{op: :x} = v) do
    [c1, c2] = v.prev
    c1 = %{c1 | grad: c1.grad + c2.data * v.grad}
    c2 = %{c2 | grad: c2.grad + c1.data * v.grad}
    %{v | prev: [c1, c2] |> b()}
  end

  defp b(%V{op: :tanh} = v) do
    [c] = v.prev
    c = %{c | grad: c.grad + (1.0 - v.data ** 2) * v.grad}
    %{v | prev: [c] |> b()}
  end

  defp b(%V{op: nil} = v), do: v
end

alias BackpropagationWithBug, as: B

# inputs
x1 = V.new(2.0, "x1")
x2 = V.new(0.0, "x2")
# weights
w1 = V.new(-3.0, "w1")
w2 = V.new(1.0, "w2")
# bias
b = V.new(6.8813735870195432, "b")

x1w1 = x1 * w1
x2w2 = x2 * w2
x1w1x2w2 = x1w1 + x2w2
n = (x1w1x2w2 + b) |> V.label("n")
o = n |> V.tanh() |> V.label("o")

o = o |> B.backward() |> V.Kino.new()
```

```elixir
# Bug symptom 1: a should have gradient 2 (a+a = 2*a)
a = V.new(3.0, "a")
b = (a + a) |> V.label("b")
b |> B.backward() |> V.Kino.new() |> Kino.render()

# Bug symptom 2: using variable more than once
a = V.new(-2.0, "a")
b = V.new(3.0, "b")
d = (a * b) |> V.label("d")
e = (a + b) |> V.label("e")
f = (d * e) |> V.label("f")

f |> B.backward() |> V.Kino.new() |> Kino.render()

:ok
```

```elixir
defmodule Backpropagation do
  alias Value, as: V

  def backward(%V{} = v) do
    v
    |> grads()
    |> put_grads(v)
  end

  def grads(%V{} = v) do
    v
    |> topo()
    |> Enum.reverse()
    |> b(%{v.id => 1.0})
  end

  defp topo(v), do: topo(v, %{}) |> elem(0)
  defp topo(%{id: id}, visited) when is_map_key(visited, id), do: {[], visited}

  defp topo(v, visited) do
    visited = Map.put(visited, v.id, true)
    {prev, visited} = Enum.flat_map_reduce(v.prev, visited, &topo/2)
    {prev ++ [v], visited}
  end

  defp b(nodes, grads)
  defp b([], grads), do: grads
  defp b([%V{op: nil} | rest], grads), do: b(rest, grads)

  defp b([%V{} = v | rest], grads) do
    c1 = Enum.at(v.prev, 0)
    c2 = Enum.at(v.prev, 1)
    grad = grads[v.id]

    new_grads =
      case v.op do
        :+ -> [{c1.id, grad}, {c2.id, grad}]
        :x -> [{c1.id, c2.data * grad}, {c2.id, c1.data * grad}]
        :tanh -> [{c1.id, (1 - v.data ** 2) * grad}]
        :exp -> [{c1.id, v.data * grad}]
        :pow -> [{c1.id, c2.data * c1.data ** (c2.data - 1) * grad}]
      end

    grads =
      Enum.reduce(new_grads, grads, fn {id, grad}, acc ->
        Map.update(acc, id, grad, &(&1 + grad))
      end)

    b(rest, grads)
  end

  defp put_grads(grads, v) do
    %{v | grad: grads[v.id] || 0.0, prev: Enum.map(v.prev, &put_grads(grads, &1))}
  end
end

alias Backpropagation, as: B

# Bug symptom 1: a should have gradient 2 (a+a = 2*a)
a = V.new(3.0, "a")
b = (a + a) |> V.label("b")
b |> B.backward() |> V.Kino.new() |> Kino.render()

# Bug symptom 2: using variable more than once
a = V.new(-2.0, "a")
b = V.new(3.0, "b")
d = (a * b) |> V.label("d")
e = (a + b) |> V.label("e")
f = (d * e) |> V.label("f")

f |> B.backward() |> V.Kino.new() |> Kino.render()

a = V.new(2.0, "a")
b = V.new(4.0, "b")
(a / b) |> B.backward() |> V.Kino.new() |> Kino.render()

:ok
```

```elixir
# inputs
x1 = V.new(2.0, "x1")
x2 = V.new(0.0, "x2")
# weights
w1 = V.new(-3.0, "w1")
w2 = V.new(1.0, "w2")
# bias
b = V.new(6.8813735870195432, "b")

x1w1 = x1 * w1
x2w2 = x2 * w2
x1w1x2w2 = x1w1 + x2w2
n = (x1w1x2w2 + b) |> V.label("n")
e = V.exp(2.0 * n)
o = ((e - 1.0) / (e + 1.0)) |> V.label("o")

o |> B.backward() |> V.Kino.new()
```

```elixir
defmodule BackpropagationWithNx do
  import Value, except: [+: 2, *: 2]
  import Nx.Defn

  @data %{x1: 2.0, w1: -3.0, x2: 0.0, w2: 1.0, b: 6.8813735870195432}

  defn(x1_grad(x1), do: grad(&my_fn(%{@data | x1: &1})).(x1))
  defn(w1_grad(w1), do: grad(&my_fn(%{@data | w1: &1})).(w1))
  defn(x2_grad(x2), do: grad(&my_fn(%{@data | x2: &1})).(x2))
  defn(w2_grad(w2), do: grad(&my_fn(%{@data | w2: &1})).(w2))

  defn my_fn(d \\ @data) do
    Nx.tanh(d.x1 * d.w1 + d.x2 * d.w2 + d.b)
  end
end

alias BackpropagationWithNx, as: BNx

BNx.my_fn() |> Nx.to_number() |> IO.inspect(label: :forward_pass)
BNx.x1_grad(2.0) |> Nx.to_number() |> IO.inspect(label: :x1)
BNx.w1_grad(-3.0) |> Nx.to_number() |> IO.inspect(label: :w1)
BNx.x2_grad(0.0) |> Nx.to_number() |> IO.inspect(label: :x2)
BNx.w2_grad(1.0) |> Nx.to_number() |> IO.inspect(label: :w2)

:ok
```

```elixir
defmodule Neuron do
  alias Value, as: V

  defstruct [:w, :b]

  def new(nin) do
    %Neuron{
      w: for(i <- 1..nin, do: V.new(rand(-1, 1), "w#{i}")),
      b: V.new(rand(-1, 1), "b")
    }
  end

  def call(%Neuron{} = n, x) do
    Enum.zip(n.w, x)
    |> Enum.map(fn {w, x} -> (x * w) |> V.label("*") end)
    |> V.sum()
    |> V.+(n.b)
  end

  def parameters(%Neuron{} = n), do: n.w ++ [n.b]

  def get_grad(%Neuron{} = n, [:w, wi], grads) do
    grads[Enum.at(n.w, wi).id]
  end

  def get_grad(%Neuron{} = n, [:b], grads) do
    grads[n.b.id]
  end

  defp rand(from, to), do: :rand.uniform_real() * (to - from) + from
end

Neuron.new(3) |> Neuron.call([1.0, 2.0, 3.0]) |> V.Kino.new() |> Kino.render()

defmodule Layer do
  defstruct [:n]

  def new(nin, nout) do
    %Layer{n: for(_ <- 1..nout, do: Neuron.new(nin))}
  end

  def call(%Layer{} = l, x) do
    for n <- l.n, do: Neuron.call(n, x)
  end

  def parameters(%Layer{} = l) do
    Enum.flat_map(l.n, &Neuron.parameters/1)
  end

  def get_grad(%Layer{} = l, [ni | path], grads) do
    Neuron.get_grad(Enum.at(l.n, ni), path, grads)
  end
end

Layer.new(3, 2)
|> Layer.call([1.0, 2.0, 3.0])

defmodule MLP do
  defstruct [:l]

  def new(nin, nouts) do
    sz = [nin | nouts]
    %MLP{l: for(o <- 1..length(nouts), do: Layer.new(Enum.at(sz, o - 1), Enum.at(sz, o)))}
  end

  def call(%MLP{} = m, x) do
    m.l
    |> Enum.reduce(x, fn l, x -> Layer.call(l, x) end)
    |> unwrap()
  end

  def parameters(%MLP{} = m) do
    Enum.flat_map(m.l, &Layer.parameters/1)
  end

  def get_grad(%MLP{} = m, [li | path], grads) do
    Layer.get_grad(Enum.at(m.l, li), path, grads)
  end

  defp unwrap([v]), do: v
  defp unwrap(vs), do: vs
end

(n = MLP.new(3, [4, 4, 1]))
|> MLP.call([2.0, 3.0, -1.0])
|> B.backward()
|> V.Kino.new()
|> Kino.render()

:ok
```

```elixir
xs = [
  [2.0, 3.0, -1.0],
  [3.0, -1.0, 0.5],
  [0.5, 1.0, 1.0],
  [1.0, 1.0, -1.0]
]

ys = [1.0, -1.0, -1.0, 1.0]
ypred = for x <- xs, do: MLP.call(n, x)
loss = for({ygt, yout} <- Enum.zip(ys, ypred), do: (yout - ygt) ** 2.0) |> V.sum()
loss = loss |> B.backward() |> IO.inspect(label: :loss)
# Graph too large to render
MLP.get_grad(n, [0, 0, :w, 0], loss |> B.grads()) |> IO.inspect(label: :w0_grad)
MLP.parameters(n) |> length()

# :ok
```
