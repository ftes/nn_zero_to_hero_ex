# Micrograd (Karpathy Neural Networks Series #1) - fork

```elixir
Mix.install(
  [
    {:axon, "~> 0.5.1"},
    {:nx, "~> 0.5"},
    {:vega_lite, "~> 0.1.6"},
    {:kino_vega_lite, "~> 0.1.7"}
  ],
  consolidate_protocols: false
)
```

## Derivatives

```elixir
# https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ

f = fn x -> 3 * x * x - 4 * x + 5 end
f.(3.0) |> IO.inspect()

alias VegaLite, as: Vl

# floating point range (step size < 1)
defmodule FRange do
  def new(from, to, step) do
    Stream.unfold(from, fn
      x when x > to -> nil
      x -> {x, x + step}
    end)
  end
end

Vl.new()
|> Vl.data_from_values(for x <- FRange.new(-5, 5, 0.1), do: %{"x" => x, y: f.(x)})
|> Vl.mark(:line)
|> Vl.encode_field(:x, "x", type: :quantitative)
|> Vl.encode_field(:y, "y", type: :quantitative)
|> Kino.render()

:ok
```

```elixir
h = 0.000001
x = 2 / 3
(f.(x + h) - f.(x)) / h
```

```elixir
a = 2.0
b = -3.0
c = 10.0
d = a * b + c
d |> IO.puts()
```

```elixir
h = 0.0001

# inputs
a = 2.0
b = -3.0
c = 10.0
d1 = a * b + c
a = a + h
d2 = a * b + c
d1 |> IO.puts()
d2 |> IO.puts()
((d2 - d1) / h) |> IO.inspect(label: :slope_a)

d1 = a * b + c
b = b + h
d2 = a * b + c
# d1 |> IO.puts()
# d2 |> IO.puts()
((d2 - d1) / h) |> IO.inspect(label: :slope_b)

d1 = a * b + c
c = c + h
d2 = a * b + c
# d1 |> IO.puts()
# d2 |> IO.puts()
((d2 - d1) / h) |> IO.inspect(label: :slope_c)
:ok
```

```elixir
defmodule Gradients do
  defstruct values: %{}

  def new(values), do: %Gradients{values: values}
end

defmodule Value do
  @derive {Inspect, only: [:data]}
  defstruct [:data, :id, :op, :label, prev: []]

  import Kernel, except: [+: 2, -: 2, /: 2, *: 2, **: 2]
  alias __MODULE__, as: V

  def new(data, attrs \\ [])
  def new(data, label) when is_binary(label), do: new(data, label: label)

  def new(data, attrs) when is_number(data) and is_list(attrs),
    do: %V{data: data + 0.0, id: System.unique_integer()} |> Map.merge(Map.new(attrs))

  def sum([%V{} = v | vs]), do: Enum.reduce(vs, v, &(&1 + &2))

  def (%V{} = a) + b when is_number(b), do: a + new(b)
  def a + (%V{} = b) when is_number(a), do: new(a) + b
  def (%V{} = a) + (%V{} = b), do: new(a.data + b.data, op: :+, prev: [a, b])
  def a + b, do: Kernel.+(a, b)

  def (%V{} = a) * b when is_number(b), do: a * new(b)
  def a * (%V{} = b) when is_number(a), do: new(a) * b
  def (%V{} = a) * (%V{} = b), do: new(a.data * b.data, op: :*, prev: [a, b])
  def a * b, do: Kernel.*(a, b)

  def tanh(a) when is_number(a), do: a |> new() |> tanh()
  def tanh(%V{} = a), do: new(:math.tanh(a.data), op: :tanh, prev: [a])

  def exp(a) when is_number(a), do: a |> new() |> exp()
  def exp(%V{} = a), do: new(:math.exp(a.data), op: :exp, prev: [a])

  def (%V{} = a) ** b when is_number(b), do: new(a.data ** b, op: :pow, prev: [a, new(b)])
  def a ** b, do: Kernel.**(a, b)

  def (%V{} = a) / b when is_number(b), do: a / new(b)
  def a / (%V{} = b) when is_number(a), do: new(a) / b
  def (%V{} = a) / (%V{} = b), do: a * b ** -1.0
  def a / b, do: Kernel./(a, b)

  def neg(%V{} = v), do: v * -1.0

  def a - (%V{} = b) when is_number(a), do: new(a) - b
  def (%V{} = a) - b when is_number(b), do: a - new(b)
  def (%V{} = a) - (%V{} = b), do: a + neg(b)
  def a - b, do: Kernel.-(a, b)

  def label(%V{} = v, label), do: %{v | label: label}

  def nudge(%V{} = v, %Gradients{} = g, factor \\ 0.01) do
    delta = g.values[v.id] * factor
    Map.update!(v, :data, &(&1 + delta))
  end
end

defmodule Value.Kino do
  alias Value, as: V
  alias Gradients, as: G

  def new(%V{} = v, %G{} = g \\ %G{}) do
    (["graph TB;"] ++ lines(v, g))
    |> Enum.join("\n")
    |> Kino.Mermaid.new()
  end

  defp lines(%{op: nil}, _), do: []

  defp lines(%V{} = v, %G{} = g) do
    [node(v, g)] ++
      Enum.map(v.prev, &edge(node(&1, g), node(v, g))) ++
      Enum.flat_map(v.prev, &lines(&1, g))
  end

  defp edge(a, b), do: "#{a} --> #{b};"

  defp node(%V{} = v, %G{} = g) do
    values =
      [v.label, v.op, Float.round(v.data, 2), gradient(g.values[v.id])]
      |> Enum.reject(&(&1 == nil))
      |> Enum.join(" ")

    "v_#{v.id}[#{values}]"
  end

  defp gradient(nil), do: nil
  defp gradient(g), do: " g #{Float.round(g, 2)}"
end
```

```elixir
import Kernel, except: [+: 2, -: 2, *: 2, /: 2, **: 2]
import Value
alias Value, as: V
alias Gradients, as: G

a = V.new(2.0, "a")
b = V.new(-3.0, "b")
c = V.new(10.0, "c")
f = V.new(-2.0, "f")
e = (a * b) |> V.label("e")
d = (e + c) |> V.label("d")
l = (d * f) |> V.label("L")

g =
  G.new(%{
    # 1. dL/dL = 1
    l.id => 1.0,
    # 2. dL/dd = f = -2
    d.id => -2.0,
    # 2. dL/df = d = 4
    f.id => 4.0,
    # 3. dL/de = dL/dd * dd/de = -2 * 1 = -2
    e.id => -2.0,
    # 3. dL/dc = dL/dd * dd/dc = -2 * 1 = -2
    c.id => -2.0,
    # 4. dL/da = dL/de * de/da = -2 * b = 6
    a.id => 6.0,
    # 4. dL/db = dL/de * de/db = -2 * a = -4
    b.id => -4.0
  })

V.Kino.new(l, g)
```

```elixir
lol = fn ->
  h = 0.0001

  a = V.new(2.0, "a")
  b = V.new(-3.0, "b")
  c = V.new(10.0, "c")
  e = (a * b) |> V.label("e")
  d = (e * c) |> V.label("d")
  f = V.new(-2.0, "f")
  l = (d * f) |> V.label("L")
  l1 = l.data

  a = V.new(2.0 + h, "a")
  b = V.new(-3.0, "b")
  c = V.new(10.0, "c")
  e = (a * b) |> V.label("e")
  d = (e + c) |> V.label("d")
  f = V.new(-2.0, "f")
  l = (d * f) |> V.label("L")
  l2 = l.data

  (l2 - l1) / h
end

lol.() |> IO.inspect(label: :grad_a)
```

```elixir
a = V.new(2.0, "a")
b = V.new(-3.0, "b")
c = V.new(10.0, "c")
f = V.new(-2.0, "f")

g =
  G.new(%{
    a.id => 6.0,
    b.id => -4.0,
    c.id => -2.0,
    f.id => 4.0
  })

a = V.nudge(a, g)
b = V.nudge(b, g)
c = V.nudge(c, g)
f = V.nudge(f, g)

e = (a * b) |> V.label("e")
d = (e + c) |> V.label("d")
l = (d * f) |> V.label("L")

IO.puts(l.data)
```

```elixir
Vl.new()
|> Vl.data_from_values(for x <- FRange.new(-5, 5, 0.1), do: %{"x" => x, y: :math.tanh(x)})
|> Vl.mark(:line)
|> Vl.encode_field(:x, "x", type: :quantitative)
|> Vl.encode_field(:y, "y", type: :quantitative)
```

```elixir
# inputs
x1 = V.new(2.0, "x1")
x2 = V.new(0.0, "x2")
# weights
w1 = V.new(-3.0, "w1")
w2 = V.new(1.0, "w2")
# bias
b = V.new(6.8813735870195432, "b")

x1w1 = x1 * w1
x2w2 = x2 * w2
x1w1x2w2 = x1w1 + x2w2
n = (x1w1x2w2 + b) |> V.label("n")
o = n |> V.tanh() |> V.label("o")

g =
  G.new(%{
    # 1. do/do = 1
    o.id => 1.0,
    # 2. do/dn = 1-tanh(o)^2 = 1-0.7^2 = 0.5
    n.id => 0.5,
    # 3. do/dx1*... after '+', just copy n.grad
    x1w1x2w2.id => 0.5,
    # 3. do/db after '+', just copy n.grad
    b.id => 0.5,
    # 4. do/x2w2 after '+', just copy x1*w1....grad
    x2w2.id => 0.5,
    # 4. do/x1w1 after '+', just copy x1*w1....grad
    x1w1.id => 0.5,
    # 5. w1.data * x1w1.grad
    x1.id => -1.5,
    # 5. do/dx2 = do/dx2*w2 * dx2*w2/dx2 = 0.5 * w2 = 0.5
    x2.id => 0.5,
    # 5. x1.data * x1w1.grad
    w1.id => 1.0,
    # 5. do/dx2 = do/dx2*w2 * dx2*w2/dx2 = 0.5 * x2 = 0
    w2.id => 0.0
  })

o |> V.Kino.new(g)
```

```elixir
defmodule TopologicalOrder do
  alias Value, as: V

  def sort(%V{} = v), do: sort(v, %{}) |> elem(0)

  defp sort(%{id: id}, visited) when is_map_key(visited, id), do: {[], visited}

  defp sort(v, visited) do
    visited = Map.put(visited, v.id, true)
    {prev, visited} = Enum.flat_map_reduce(v.prev, visited, &sort/2)
    {prev ++ [v], visited}
  end
end

defmodule BackpropagationWithBug do
  alias Value, as: V

  def run(%V{} = v) do
    v
    |> TopologicalOrder.sort()
    |> Enum.reverse()
    |> run(%{v.id => 1.0})
    |> Gradients.new()
  end

  defp run(nodes, gradients)
  defp run([], gradients), do: gradients
  defp run([%V{op: nil} | rest], gradients), do: run(rest, gradients)

  defp run([%V{} = v | rest], gradients) do
    c1 = Enum.at(v.prev, 0)
    c2 = Enum.at(v.prev, 1)
    grad = gradients[v.id]

    new_gradients =
      case v.op do
        :+ -> %{c1.id => grad, c2.id => grad}
        :* -> %{c1.id => c2.data * grad, c2.id => c1.data * grad}
        :tanh -> %{c1.id => (1 - v.data ** 2) * grad}
        :exp -> %{c1.id => v.data * grad}
        :pow -> %{c1.id => c2.data * c1.data ** (c2.data - 1) * grad}
      end

    gradients = Map.merge(gradients, new_gradients)

    run(rest, gradients)
  end
end

alias BackpropagationWithBug, as: B

# inputs
x1 = V.new(2.0, "x1")
x2 = V.new(0.0, "x2")
# weights
w1 = V.new(-3.0, "w1")
w2 = V.new(1.0, "w2")
# bias
b = V.new(6.8813735870195432, "b")

x1w1 = x1 * w1
x2w2 = x2 * w2
x1w1x2w2 = x1w1 + x2w2
n = (x1w1x2w2 + b) |> V.label("n")
o = n |> V.tanh() |> V.label("o")
g = B.run(o)

V.Kino.new(o, g)
```

```elixir
# Bug symptom 1: a should have gradient 2 (a+a = 2*a)
a = V.new(3.0, "a")
b = (a + a) |> V.label("b")
g = b |> B.run()
V.Kino.new(b, g) |> Kino.render()

# Bug symptom 2: using variable more than once
a = V.new(-2.0, "a")
b = V.new(3.0, "b")
d = (a * b) |> V.label("d")
e = (a + b) |> V.label("e")
f = (d * e) |> V.label("f")

g = f |> B.run()
V.Kino.new(f, g) |> Kino.render()

:ok
```

```elixir
defmodule Backpropagation do
  alias Value, as: V

  def run(%V{} = v) do
    v
    |> TopologicalOrder.sort()
    |> Enum.reverse()
    |> run(%{v.id => 1.0})
    |> Gradients.new()
  end

  defp run(nodes, gradients)
  defp run([], gradients), do: gradients
  defp run([%V{op: nil} | rest], gradients), do: run(rest, gradients)

  defp run([%V{} = v | rest], gradients) do
    c1 = Enum.at(v.prev, 0)
    c2 = Enum.at(v.prev, 1)
    grad = gradients[v.id]

    new_gradients =
      case v.op do
        :+ -> [{c1.id, grad}, {c2.id, grad}]
        :* -> [{c1.id, c2.data * grad}, {c2.id, c1.data * grad}]
        :tanh -> [{c1.id, (1 - v.data ** 2) * grad}]
        :exp -> [{c1.id, v.data * grad}]
        :pow -> [{c1.id, c2.data * c1.data ** (c2.data - 1) * grad}]
      end

    gradients =
      Enum.reduce(new_gradients, gradients, fn {id, grad}, acc ->
        Map.update(acc, id, grad, &(&1 + grad))
      end)

    run(rest, gradients)
  end
end

alias Backpropagation, as: B

# Bug symptom 1: a should have gradient 2 (a+a = 2*a)
a = V.new(3.0, "a")
b = (a + a) |> V.label("b")
g = b |> B.run()
V.Kino.new(b, g) |> Kino.render()

# Bug symptom 2: using variable more than once
a = V.new(-2.0, "a")
b = V.new(3.0, "b")
d = (a * b) |> V.label("d")
e = (a + b) |> V.label("e")
f = (d * e) |> V.label("f")

g = f |> B.run()
V.Kino.new(f, g) |> Kino.render()

a = V.new(2.0, "a")
b = V.new(4.0, "b")
c = a / b
g = c |> B.run()
V.Kino.new(c, g) |> Kino.render()

:ok
```

```elixir
# inputs
x1 = V.new(2.0, "x1")
x2 = V.new(0.0, "x2")
# weights
w1 = V.new(-3.0, "w1")
w2 = V.new(1.0, "w2")
# bias
b = V.new(6.8813735870195432, "b")

x1w1 = x1 * w1
x2w2 = x2 * w2
x1w1x2w2 = x1w1 + x2w2
n = (x1w1x2w2 + b) |> V.label("n")
e = V.exp(2.0 * n)
o = ((e - 1.0) / (e + 1.0)) |> V.label("o")

g = o |> B.run()
V.Kino.new(o, g)
```

```elixir
defmodule BackpropagationWithNx do
  import Value, except: [+: 2, *: 2]
  import Nx.Defn

  @data %{x1: 2.0, w1: -3.0, x2: 0.0, w2: 1.0, b: 6.8813735870195432}

  defn(x1_grad(x1), do: grad(&my_fn(%{@data | x1: &1})).(x1))
  defn(w1_grad(w1), do: grad(&my_fn(%{@data | w1: &1})).(w1))
  defn(x2_grad(x2), do: grad(&my_fn(%{@data | x2: &1})).(x2))
  defn(w2_grad(w2), do: grad(&my_fn(%{@data | w2: &1})).(w2))

  defn my_fn(d \\ @data) do
    Nx.tanh(d.x1 * d.w1 + d.x2 * d.w2 + d.b)
  end
end

alias BackpropagationWithNx, as: BNx

BNx.my_fn() |> Nx.to_number() |> IO.inspect(label: :forward_pass)
BNx.x1_grad(2.0) |> Nx.to_number() |> IO.inspect(label: :x1)
BNx.w1_grad(-3.0) |> Nx.to_number() |> IO.inspect(label: :w1)
BNx.x2_grad(0.0) |> Nx.to_number() |> IO.inspect(label: :x2)
BNx.w2_grad(1.0) |> Nx.to_number() |> IO.inspect(label: :w2)

:ok
```

```elixir
defmodule Neuron do
  alias Value, as: V

  defstruct [:w, :b]

  def new(nin) do
    %Neuron{
      w: for(i <- 1..nin, do: V.new(rand(-1, 1), "w#{i}")),
      b: V.new(rand(-1, 1), "b")
    }
  end

  def call(%Neuron{} = n, x) do
    Enum.zip(n.w, x)
    |> Enum.map(fn {w, x} -> x * w end)
    |> V.sum()
    |> V.+(n.b)
  end

  def minimize_loss(%Neuron{} = n, %Gradients{} = g, factor) do
    %Neuron{
      w: Enum.map(n.w, &V.nudge(&1, g, factor)),
      b: V.nudge(n.b, g, factor)
    }
  end

  defp rand(from, to), do: :rand.uniform_real() * (to - from) + from
end

Neuron.new(3) |> Neuron.call([1.0, 2.0, 3.0]) |> V.Kino.new()
```

```elixir
defmodule Layer do
  defstruct [:n]

  def new(nin, nout) do
    %Layer{n: for(_ <- 1..nout, do: Neuron.new(nin))}
  end

  def call(%Layer{} = l, x) do
    for n <- l.n, do: Neuron.call(n, x)
  end

  def minimize_loss(%Layer{} = l, %Gradients{} = g, factor) do
    %Layer{n: Enum.map(l.n, &Neuron.minimize_loss(&1, g, factor))}
  end
end

Layer.new(3, 2)
|> Layer.call([1.0, 2.0, 3.0])
```

```elixir
defmodule MLP do
  defstruct [:l]

  def new(nin, nouts) do
    sz = [nin | nouts]
    %MLP{l: for(o <- 1..length(nouts), do: Layer.new(Enum.at(sz, o - 1), Enum.at(sz, o)))}
  end

  def call(%MLP{} = m, x) do
    m.l
    |> Enum.reduce(x, fn l, x -> Layer.call(l, x) end)
    |> unwrap()
  end

  def minimize_loss(%MLP{} = m, %Gradients{} = g, factor \\ -0.01) do
    %MLP{l: Enum.map(m.l, &Layer.minimize_loss(&1, g, factor))}
  end

  defp unwrap([v]), do: v
  defp unwrap(vs), do: vs
end

n = MLP.new(3, [4, 4, 1])
o = n |> MLP.call([2.0, 3.0, -1.0])
g = B.run(o)
V.Kino.new(o, g)
```

```elixir
xs = [
  [2.0, 3.0, -1.0],
  [3.0, -1.0, 0.5],
  [0.5, 1.0, 1.0],
  [1.0, 1.0, -1.0]
]

ys = [1.0, -1.0, -1.0, 1.0]
ypred = for(x <- xs, do: MLP.call(n, x))

loss =
  for({ygt, yout} <- Enum.zip(ys, ypred), do: (yout - ygt) ** 2.0)
  |> V.sum()
  |> IO.inspect(label: :loss)

g = loss |> B.run()
# V.Kino.new(loss, g) |> Kino.render()
# Graph too large to render
n = MLP.minimize_loss(n, g, -0.01)

ypred = for(x <- xs, do: MLP.call(n, x))

loss =
  for({ygt, yout} <- Enum.zip(ys, ypred), do: (yout - ygt) ** 2.0)
  |> V.sum()
  |> IO.inspect(label: :loss)

g = loss |> B.run()
n = MLP.minimize_loss(n, g, -0.01)

ypred = for(x <- xs, do: MLP.call(n, x))

loss =
  for({ygt, yout} <- Enum.zip(ys, ypred), do: (yout - ygt) ** 2.0)
  |> V.sum()
  |> IO.inspect(label: :loss)

g = loss |> B.run()
n = MLP.minimize_loss(n, g, -0.01)

ypred = for(x <- xs, do: MLP.call(n, x))

loss =
  for({ygt, yout} <- Enum.zip(ys, ypred), do: (yout - ygt) ** 2.0)
  |> V.sum()
  |> IO.inspect(label: :loss)

g = loss |> B.run()
n = MLP.minimize_loss(n, g, -0.01)

ypred = for(x <- xs, do: MLP.call(n, x))

loss =
  for({ygt, yout} <- Enum.zip(ys, ypred), do: (yout - ygt) ** 2.0)
  |> V.sum()
  |> IO.inspect(label: :loss)

g = loss |> B.run()
n = MLP.minimize_loss(n, g, -0.01)

ypred = for(x <- xs, do: MLP.call(n, x))

loss =
  for({ygt, yout} <- Enum.zip(ys, ypred), do: (yout - ygt) ** 2.0)
  |> V.sum()
  |> IO.inspect(label: :loss)

# g = loss |> B.run()
# n = MLP.minimize_loss(n, g, -0.01)

:ok
```

```elixir
defmodule GradientDescent do
  def run(%MLP{} = net, xs, ys, count \\ 100, factor \\ -0.01) do
    Enum.reduce(1..count, net, fn _, net ->
      # forward pass
      ypred = for(x <- xs, do: MLP.call(net, x))
      loss = for({ygt, yout} <- Enum.zip(ys, ypred), do: (yout - ygt) ** 2.0) |> V.sum()

      # backward pass
      # Don't have to flush the grad, because we're functional :)
      gradients = loss |> B.run()

      # update
      MLP.minimize_loss(net, gradients, factor)
    end)
  end
end

xs = [
  [2.0, 3.0, -1.0],
  [3.0, -1.0, 0.5],
  [0.5, 1.0, 1.0],
  [1.0, 1.0, -1.0]
]

ys = [1.0, -1.0, -1.0, 1.0]
n = MLP.new(3, [4, 4, 1])
n = GradientDescent.run(n, xs, ys, 1000, -0.001)
for x <- xs, do: MLP.call(n, x)
```
