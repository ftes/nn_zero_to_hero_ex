# Makemore #1 (Karpathy Neural Networks Series #2)

```elixir
Mix.install(
  [
    {:nx, "~> 0.5.1"},
    {:exla, "~> 0.5.1"},
    {:axon, "~> 0.5.1"},
    {:kino, "~> 0.8.1"}
  ],
  system_env: [
    # {"XLA_TARGET", "cuda118"},
  ],
  config: [
    nx: [default_backend: EXLA.Backend]
  ]
)
```

## Section

```elixir
# https://www.youtube.com/watch?v=PaCmpygFfXo&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=2

words =
  Path.join(__DIR__, 'names.txt')
  |> File.read!()
  |> String.split("\n")

Enum.slice(words, 1..10)
```

```elixir
word_count = length(words)
```

```elixir
Enum.min_max_by(words, &String.length/1)
```

```elixir
bigram_counts =
  words
  |> Enum.map(&String.graphemes(".#{&1}."))
  |> Enum.flat_map(&Enum.zip(&1, Enum.slice(&1, 1..-1)))
  |> Enum.frequencies()

bigram_counts
|> Enum.sort_by(&(-elem(&1, 1)))
|> Enum.slice(0..10)
```

```elixir
chars = words |> Enum.join("") |> String.graphemes() |> MapSet.new() |> Enum.sort()
chars = ["." | chars]
stoi = chars |> Enum.with_index() |> Map.new()
itos = Map.new(stoi, fn {c, i} -> {i, c} end)
:ok
```

```elixir
n_data = for from <- chars, do: for(to <- chars, do: bigram_counts[{from, to}] || 0)
n = Nx.tensor(n_data, names: [:from, :to], type: :u32)
```

```elixir
n |> Nx.to_heatmap(ansi_enabled: true)
```

```elixir
key = Nx.Random.key(424_242)
p = n[from: 0] |> Nx.divide(Nx.sum(n[from: 0])) |> IO.inspect()
{sample, _key} = Nx.Random.choice(key, Nx.iota({27}), p, samples: 1, replace: true)
sample = sample |> Nx.reshape({}) |> Nx.to_number()
itos[sample]
```

```elixir
sums = Nx.sum(n, axes: [:to], keep_axes: true)
probs = n |> Nx.divide(sums)
probs[from: 0] |> Nx.sum()
```

```elixir
n_plus_1 = n |> Nx.add(1)
sums = Nx.sum(n_plus_1, axes: [:to], keep_axes: true)
probs_smoothed = n_plus_1 |> Nx.divide(sums)
```

```elixir
defmodule Sample do
  def take(probs, itos, seed \\ 999) do
    word(probs, Nx.Random.key(seed))
    |> Enum.reverse()
    |> Enum.map(&itos[&1])
    |> Enum.join()
    |> String.trim(".")
  end

  defp word(probs, key, acc \\ [0])
  defp word(_, _, acc) when length(acc) > 100, do: acc
  defp word(_, _, [0, _ | _] = acc), do: acc

  defp word(probs, key, [ix | _] = acc) do
    p = probs[ix]
    {ix, key} = Nx.Random.choice(key, Nx.iota({27}), p)
    ix = ix |> Nx.reshape({}) |> Nx.to_number()
    word(probs, key, [ix | acc])
  end
end

for i <- 1..5, do: Sample.take(probs_smoothed, itos, i)
```

```elixir
# GOAL: maximize likelihood of the data w.r.t. model parameters (statistical modeling)
# equivalent to maximizing the log likelihood (because log is monotonic)
# equivalent to minimizing the negative log likelihood
# equivalent to minimizing the average negative log likelihood

# log(a*b*c) = log(a) + log(b) + log(c)
```

```elixir
# ["andrejq"]
{log_likelihood, bigram_count} =
  for w <- words,
      chs <- [String.graphemes(".#{w}.")],
      {ch1, ch2} <- Enum.zip(chs, Enum.slice(chs, 1..-1)),
      reduce: {0.0, 0} do
    {log_likelihood, bigram_count} ->
      ix1 = stoi[ch1]
      ix2 = stoi[ch2]
      prob = probs_smoothed[from: ix1, to: ix2]
      logprob = Nx.log(prob) |> Nx.to_number()
      {log_likelihood + logprob, bigram_count + 1}
  end
  |> IO.inspect()

nll = -log_likelihood |> IO.inspect(label: :negative_log_likelihood)
average_nll = (nll / word_count) |> IO.inspect(label: :average)

:ok
```

```elixir
defmodule OneHot do
  import Nx.Defn

  defn enc(input, opts \\ []) do
    opts = keyword!(opts, count: 1)

    input
    |> Nx.new_axis(-1)
    |> Nx.equal(Nx.iota({opts[:count]}, type: :u8))
  end
end

# create training set of bigrams {x, y}
{xs, ys} =
  for w <- Enum.slice(words, 0..0),
      chs <- [String.graphemes(".#{w}.")],
      {ch1, ch2} <- Enum.zip(chs, Enum.slice(chs, 1..-1)),
      reduce: {[], []} do
    {xs, ys} ->
      ix1 = stoi[ch1]
      ix2 = stoi[ch2]
      {[ix1 | xs], [ix2 | ys]}
  end

xs = xs |> Enum.reverse() |> Nx.tensor(names: [:i], type: :u8)
ys = ys |> Enum.reverse() |> Nx.tensor(names: [:i], type: :u8)
training_set_size = Nx.size(xs) |> IO.inspect(label: :training_set_size)

xenc = OneHot.enc(xs, count: 27)
yenc = OneHot.enc(ys, count: 27)

Enum.zip(
  xs |> Nx.to_list() |> Enum.map(&itos[&1]),
  ys |> Nx.to_list() |> Enum.map(&itos[&1])
)
|> IO.inspect()

Nx.stack({xenc, yenc})[i: 0..4] |> Nx.to_heatmap()
```

```elixir
key = Nx.Random.key(9)
{w, _key} = Nx.Random.normal(key, shape: {27, 27})
logits = xenc |> Nx.dot(w)

# softmax
counts = logits |> Nx.exp()
probs = counts |> Nx.divide(Nx.sum(counts, axes: [1], keep_axes: true)) |> IO.inspect()

probs[0] |> Nx.sum()
```

```elixir
defmodule SoftMax do
  import Nx.Defn

  defn activate(logits) do
    # equivalent to n matrix
    counts =
      logits
      |> Nx.exp()
      |> Nx.rename([:i, :counts])

    counts
    |> Nx.divide(Nx.sum(counts, axes: [:counts], keep_axes: true))
    |> Nx.rename([:i, :probs])
  end
end

probs = SoftMax.activate(logits)
probs[i: 0] |> Nx.sum()
```

```elixir
# average negative log likelhood
prob_indices = Nx.stack({Nx.iota({training_set_size}), ys[0..(training_set_size - 1)]}, axis: -1)

loss =
  probs
  |> Nx.gather(prob_indices)
  |> Nx.log()
  |> Nx.mean()
  |> Nx.negate()
```

```elixir
defmodule BigramNet do
  import Nx.Defn

  defn loss(w, xenc, ys) do
    probs = forward_pass(w, xenc)
    probs_indices = Nx.stack({Nx.iota({Nx.size(ys)}, type: :u8), ys}, axis: -1)

    probs
    |> Nx.gather(probs_indices)
    |> Nx.log()
    |> Nx.mean()
    |> Nx.negate()
    |> regularize(w)
  end

  # push w's towards 0
  defn regularize(loss, w) do
    loss + 0.01 * Nx.mean(w ** 2)
  end

  defn forward_pass(w, xenc) do
    logits = xenc |> Nx.dot(w)
    counts = logits |> Nx.exp()
    _probs = counts / Nx.sum(counts, axes: [1], keep_axes: true)
  end

  defn backward_pass(w, xenc, ys) do
    grad(w, fn w -> loss(w, xenc, ys) end)
  end
end

# create training set of bigrams {x, y}
{xs, ys} =
  for w <- words,
      chs <- [String.graphemes(".#{w}.")],
      {ch1, ch2} <- Enum.zip(chs, Enum.slice(chs, 1..-1)),
      reduce: {[], []} do
    {xs, ys} ->
      ix1 = stoi[ch1]
      ix2 = stoi[ch2]
      {[ix1 | xs], [ix2 | ys]}
  end

xs = xs |> Enum.reverse() |> Nx.tensor(type: :u8)
ys = ys |> Enum.reverse() |> Nx.tensor(type: :u8)
xenc = OneHot.enc(xs, count: 27)
yenc = OneHot.enc(ys, count: 27)

key = Nx.Random.key(1)
{w, _key} = Nx.Random.normal(key, shape: {27, 27})
# w = Nx.broadcast(0, {27, 27})

BigramNet.loss(w, xenc, ys) |> Nx.to_number() |> IO.inspect(label: :loss)

# gradient descent
# TODO Loss much higher than 2.45 after 100 iterations
w =
  for _ <- 1..100, reduce: w do
    w ->
      g = BigramNet.backward_pass(w, xenc, ys)
      _w = w |> Nx.subtract(Nx.multiply(50, g))
  end

# w[stoi["m"]] |> Nx.to_list() |> Enum.map(&Float.round(&1, 2)) |> IO.inspect()

BigramNet.loss(w, xenc, ys) |> Nx.to_number() |> IO.inspect(label: :loss)

:ok
```

```elixir
w_counts = w |> Nx.exp()
w_probs = w_counts |> Nx.divide(w_counts |> Nx.sum(axes: [1], keep_axes: true))
w_probs[stoi["m"]] |> IO.inspect()

# TODO w != n
diff = w_probs |> Nx.subtract(probs_smoothed) |> Nx.abs() |> IO.inspect()

diff
|> Nx.reshape({27 * 27})
|> Nx.argsort(direction: :desc)
|> Nx.to_list()
|> Enum.map(&{div(&1, 27), rem(&1, 27)})
|> Enum.map(fn {from, to} ->
  {itos[from], itos[to], w_probs[from][to] |> Nx.to_number(),
   probs_smoothed[from][to] |> Nx.to_number()}
end)
```

```elixir
defmodule SampleNet do
  def take(w, itos, seed \\ :rand.uniform(9_999_999)) do
    word(w, Nx.Random.key(seed))
    |> Enum.map(&itos[&1])
    |> Enum.join()
    |> String.trim(".")
  end

  def word(w, key, acc \\ [0])
  def word(_, _, acc) when length(acc) > 100, do: Enum.reverse(acc)
  def word(_, _, [0 | _] = acc) when length(acc) > 1, do: Enum.reverse(acc)

  def word(w, key, [last | _] = acc) do
    xenc = OneHot.enc(last, count: 27)
    logits = xenc |> Nx.dot(w)
    counts = Nx.exp(logits)
    p = counts |> Nx.divide(Nx.sum(counts, axes: [0], keep_axes: true))
    {c, key} = Nx.Random.choice(key, Nx.iota({27}), p, samples: 1)
    c = c |> Nx.reshape({}) |> Nx.to_number()

    word(w, key, [c | acc])
  end
end

for i <- 1..10, do: SampleNet.take(w, itos, i)
```

## Addon: Use Axon instead of custom implementation

```elixir
alias Axon, as: A

train_data =
  Stream.zip(
    Nx.to_batched(xenc, 32),
    Nx.to_batched(yenc, 32)
  )

model =
  A.input("data")
  |> A.dense(27, activation: :softmax, use_bias: false)

trained =
  model
  |> A.Loop.trainer(:categorical_cross_entropy, A.Optimizers.adam(0.1))
  |> A.Loop.run(train_data, %{}, iterations: 100, epochs: 10, compiler: EXLA)

:ok
```

```elixir
w = trained["dense_0"]["kernel"]
w_count = w |> Nx.exp()
w_count |> Nx.divide(Nx.sum(w_count, axes: [1], keep_axes: true)) |> IO.inspect()
probs_smoothed |> IO.inspect()
:ok
```

```elixir
{_, predict_fn} = A.build(model)
# predict_fn.(trained, xenc)

defmodule SampleAxon do
  def take(predict_fn, itos, seed \\ :rand.uniform(9_999_999)) do
    word(predict_fn, Nx.Random.key(seed))
    |> Enum.map(&itos[&1])
    |> Enum.join()
    |> String.trim(".")
  end

  def word(predict_fn, key, acc \\ [0])
  def word(_, _, acc) when length(acc) > 100, do: Enum.reverse(acc)
  def word(_, _, [0 | _] = acc) when length(acc) > 1, do: Enum.reverse(acc)

  def word(predict_fn, key, [last | _] = acc) do
    xenc = OneHot.enc(last, count: 27) |> Nx.reshape({1, 27})
    p = predict_fn.(xenc)[0]
    {c, key} = Nx.Random.choice(key, Nx.iota({27}), p, samples: 1)
    c = c |> Nx.reshape({}) |> Nx.to_number()

    word(predict_fn, key, [c | acc])
  end
end

for i <- 1..10, do: SampleAxon.take(&predict_fn.(trained, &1), itos, i)
```
